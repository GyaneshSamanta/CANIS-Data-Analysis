{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "fake_df = pd.read_csv(r\"Dataset/DataSet_Misinfo_FAKE.csv\")\n",
    "true_df = pd.read_csv(r\"Dataset/DataSet_Misinfo_TRUE.csv\")\n",
    "propaganda_df = pd.read_csv(r\"Dataset/EXTRA_RussianPropagandaSubset.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "import nltk\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary statistics for DataSet_Misinfo_FAKE.csv:\n",
      "         Unnamed: 0\n",
      "count  43642.000000\n",
      "mean   22293.806173\n",
      "std    12889.800176\n",
      "min        0.000000\n",
      "25%    10910.250000\n",
      "50%    22450.500000\n",
      "75%    33472.750000\n",
      "max    44426.000000\n"
     ]
    }
   ],
   "source": [
    "# Summary statistics for fake_df\n",
    "print(\"Summary statistics for DataSet_Misinfo_FAKE.csv:\")\n",
    "print(fake_df.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Summary statistics for DataSet_Misinfo_TRUE.csv:\n",
      "         Unnamed: 0\n",
      "count  34975.000000\n",
      "mean   17487.000000\n",
      "std    10096.557169\n",
      "min        0.000000\n",
      "25%     8743.500000\n",
      "50%    17487.000000\n",
      "75%    26230.500000\n",
      "max    34974.000000\n"
     ]
    }
   ],
   "source": [
    "# Summary statistics for true_df\n",
    "print(\"\\nSummary statistics for DataSet_Misinfo_TRUE.csv:\")\n",
    "print(true_df.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Summary statistics for EXTRA_RussianPropagandaSubset.csv:\n",
      "       Unnamed: 0\n",
      "count   7369.0000\n",
      "mean    3684.0000\n",
      "std     2127.3914\n",
      "min        0.0000\n",
      "25%     1842.0000\n",
      "50%     3684.0000\n",
      "75%     5526.0000\n",
      "max     7368.0000\n"
     ]
    }
   ],
   "source": [
    "# Summary statistics for propaganda_df\n",
    "print(\"\\nSummary statistics for EXTRA_RussianPropagandaSubset.csv:\")\n",
    "print(propaganda_df.describe())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding a new column \"Label\" with \"true\" for true_df and \"fake\" for fake_df\n",
    "true_df[\"Label\"] = \"true\"\n",
    "fake_df[\"Label\"] = \"fake\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Unnamed: 0                                               text Label\n",
      "0           0  The head of a conservative Republican faction ...  true\n",
      "1           1  Transgender people will be allowed for the fir...  true\n",
      "2           2  The special counsel investigation of links bet...  true\n",
      "3           3  Trump campaign adviser George Papadopoulos tol...  true\n",
      "4           4  President Donald Trump called on the U.S. Post...  true\n"
     ]
    }
   ],
   "source": [
    "# Concatenating the two dataframes into a single dataframe\n",
    "combined_df = pd.concat([true_df, fake_df], ignore_index=True)\n",
    "\n",
    "# Print the first few rows of the combined dataframe to verify the new column has been added\n",
    "print(combined_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text Label\n",
      "0  The head of a conservative Republican faction ...  true\n",
      "1  Transgender people will be allowed for the fir...  true\n",
      "2  The special counsel investigation of links bet...  true\n",
      "3  Trump campaign adviser George Papadopoulos tol...  true\n",
      "4  President Donald Trump called on the U.S. Post...  true\n"
     ]
    }
   ],
   "source": [
    "# Removing the index column from the combined_df dataframe\n",
    "combined_df = combined_df.drop(columns=[\"Unnamed: 0\"])\n",
    "\n",
    "# Print the first few rows of the updated combined dataframe to verify the index column has been removed\n",
    "print(combined_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df['Label'] = combined_df['Label'].replace({'true': True, 'fake': False})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df['text'] = combined_df['text'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "import re\n",
    "\n",
    "# Add a new column to store the count of stop words in every row of the dataset in the 'text' column\n",
    "stop_words = set(stopwords.words('english'))\n",
    "combined_df['stop_words_count'] = combined_df['text'].apply(lambda x: len([word for word in word_tokenize(x) if word.lower() in stop_words]))\n",
    "\n",
    "# # Stem the words in the 'text' column\n",
    "# stemmer = PorterStemmer()\n",
    "# combined_df['stemmed_text'] = combined_df['text'].apply(lambda x: ' '.join(stemmer.stem(word) for word in word_tokenize(x)))\n",
    "\n",
    "# Map part of speech tags to WordNet tags\n",
    "# def get_wordnet_pos(word):\n",
    "#     \"\"\"Map POS tag to first character used by WordNetLemmatizer\"\"\"\n",
    "#     tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "#     tag_dict = {\"J\": wordnet.ADJ,\n",
    "#                 \"N\": wordnet.NOUN,\n",
    "#                 \"V\": wordnet.VERB,\n",
    "#                 \"R\": wordnet.ADV}\n",
    "#     return tag_dict.get(tag, wordnet.NOUN)\n",
    "\n",
    "# # Lemmatize the words in the 'text' column\n",
    "# lemmatizer = WordNetLemmatizer()\n",
    "# combined_df['lemmatized_text'] = combined_df['text'].apply(lambda x: ' '.join(lemmatizer.lemmatize(word, get_wordnet_pos(word)) for word in word_tokenize(x)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word count\n",
    "combined_df['word_count'] = combined_df['text'].apply(lambda x: len(str(x).split(\" \")))\n",
    "\n",
    "# Capitalization count\n",
    "combined_df['cap_count'] = combined_df['text'].apply(lambda x: sum(1 for c in x if c.isupper()))\n",
    "\n",
    "# Punctuation count\n",
    "combined_df['punct_count'] = combined_df['text'].apply(lambda x: len([c for c in x if c in string.punctuation]))\n",
    "\n",
    "# Number of exclamation marks in headline or summary\n",
    "combined_df['num_exclamations'] = combined_df['text'].apply(lambda x: x.count('!')) + combined_df['text'].apply(lambda x: x.count('!'))\n",
    "\n",
    "# Number of capitalized words in headline or summary\n",
    "combined_df['cap_words_headline'] = combined_df['text'].apply(lambda x: sum(1 for c in x if c.isupper())) \n",
    "combined_df['cap_words_summary'] = combined_df['text'].apply(lambda x: sum(1 for c in x if c.isupper()))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>Label</th>\n",
       "      <th>stop_words_count</th>\n",
       "      <th>word_count</th>\n",
       "      <th>cap_count</th>\n",
       "      <th>punct_count</th>\n",
       "      <th>num_exclamations</th>\n",
       "      <th>cap_words_headline</th>\n",
       "      <th>cap_words_summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The head of a conservative Republican faction ...</td>\n",
       "      <td>True</td>\n",
       "      <td>303</td>\n",
       "      <td>749</td>\n",
       "      <td>144</td>\n",
       "      <td>115</td>\n",
       "      <td>0</td>\n",
       "      <td>144</td>\n",
       "      <td>144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Transgender people will be allowed for the fir...</td>\n",
       "      <td>True</td>\n",
       "      <td>171</td>\n",
       "      <td>396</td>\n",
       "      <td>72</td>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "      <td>72</td>\n",
       "      <td>72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The special counsel investigation of links bet...</td>\n",
       "      <td>True</td>\n",
       "      <td>201</td>\n",
       "      <td>462</td>\n",
       "      <td>106</td>\n",
       "      <td>44</td>\n",
       "      <td>0</td>\n",
       "      <td>106</td>\n",
       "      <td>106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Trump campaign adviser George Papadopoulos tol...</td>\n",
       "      <td>True</td>\n",
       "      <td>153</td>\n",
       "      <td>373</td>\n",
       "      <td>99</td>\n",
       "      <td>48</td>\n",
       "      <td>0</td>\n",
       "      <td>99</td>\n",
       "      <td>99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>President Donald Trump called on the U.S. Post...</td>\n",
       "      <td>True</td>\n",
       "      <td>348</td>\n",
       "      <td>860</td>\n",
       "      <td>182</td>\n",
       "      <td>132</td>\n",
       "      <td>2</td>\n",
       "      <td>182</td>\n",
       "      <td>182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78612</th>\n",
       "      <td>The USA wants to divide Syria.\\r\\n\\r\\nGreat Br...</td>\n",
       "      <td>False</td>\n",
       "      <td>36</td>\n",
       "      <td>70</td>\n",
       "      <td>26</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>26</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78613</th>\n",
       "      <td>The Ukrainian coup d'etat cost the US nothing ...</td>\n",
       "      <td>False</td>\n",
       "      <td>40</td>\n",
       "      <td>106</td>\n",
       "      <td>38</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>38</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78614</th>\n",
       "      <td>The European Parliament falsifies history by d...</td>\n",
       "      <td>False</td>\n",
       "      <td>61</td>\n",
       "      <td>129</td>\n",
       "      <td>28</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>28</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78615</th>\n",
       "      <td>The European Parliament falsifies history by d...</td>\n",
       "      <td>False</td>\n",
       "      <td>61</td>\n",
       "      <td>129</td>\n",
       "      <td>28</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>28</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78616</th>\n",
       "      <td>A leading FSB officer, Segey Beseda, said duri...</td>\n",
       "      <td>False</td>\n",
       "      <td>24</td>\n",
       "      <td>55</td>\n",
       "      <td>26</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>26</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>78617 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  Label  \\\n",
       "0      The head of a conservative Republican faction ...   True   \n",
       "1      Transgender people will be allowed for the fir...   True   \n",
       "2      The special counsel investigation of links bet...   True   \n",
       "3      Trump campaign adviser George Papadopoulos tol...   True   \n",
       "4      President Donald Trump called on the U.S. Post...   True   \n",
       "...                                                  ...    ...   \n",
       "78612  The USA wants to divide Syria.\\r\\n\\r\\nGreat Br...  False   \n",
       "78613  The Ukrainian coup d'etat cost the US nothing ...  False   \n",
       "78614  The European Parliament falsifies history by d...  False   \n",
       "78615  The European Parliament falsifies history by d...  False   \n",
       "78616  A leading FSB officer, Segey Beseda, said duri...  False   \n",
       "\n",
       "       stop_words_count  word_count  cap_count  punct_count  num_exclamations  \\\n",
       "0                   303         749        144          115                 0   \n",
       "1                   171         396         72           40                 0   \n",
       "2                   201         462        106           44                 0   \n",
       "3                   153         373         99           48                 0   \n",
       "4                   348         860        182          132                 2   \n",
       "...                 ...         ...        ...          ...               ...   \n",
       "78612                36          70         26           14                 0   \n",
       "78613                40         106         38           13                 0   \n",
       "78614                61         129         28           16                 0   \n",
       "78615                61         129         28           16                 0   \n",
       "78616                24          55         26            6                 0   \n",
       "\n",
       "       cap_words_headline  cap_words_summary  \n",
       "0                     144                144  \n",
       "1                      72                 72  \n",
       "2                     106                106  \n",
       "3                      99                 99  \n",
       "4                     182                182  \n",
       "...                   ...                ...  \n",
       "78612                  26                 26  \n",
       "78613                  38                 38  \n",
       "78614                  28                 28  \n",
       "78615                  28                 28  \n",
       "78616                  26                 26  \n",
       "\n",
       "[78617 rows x 9 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial comparision of labels and stop words\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "# Create a violinplot to visualize the distribution of stop words count with respect to the label column\n",
    "sns.violinplot(x='Label', y='stop_words_count', data=combined_df)\n",
    "\n",
    "# Set the x-label and y-label\n",
    "plt.xlabel('Label')\n",
    "plt.ylabel('Stop Words Count')\n",
    "\n",
    "# Set the plot title\n",
    "plt.title('Stop Words Count vs Label')\n",
    "\n",
    "# Save the plot as a PNG file\n",
    "plt.savefig('stop_words_count_vs_label.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's look deeper into the distribution of the previous plot\n",
    "\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create a scatter plot to visualize the relationship between word count and stop words count with respect to the label column\n",
    "sns.scatterplot(x='word_count', y='stop_words_count', hue='Label', data=combined_df)\n",
    "\n",
    "# Set the x-label and y-label\n",
    "plt.xlabel('Word Count')\n",
    "plt.ylabel('Stop Words Count')\n",
    "\n",
    "# Set the plot title\n",
    "plt.title('Word Count vs Stop Words Count, by Label')\n",
    "\n",
    "# Save the plot as a PNG file\n",
    "plt.savefig('word_count_vs_stop_words_count.png')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True news stop words ratio: 40.74%\n",
      "Fake news stop words ratio: 42.58%\n"
     ]
    }
   ],
   "source": [
    "# Calculate the total number of words and stop words in each text\n",
    "combined_df['total_words'] = combined_df['word_count'] + combined_df['num_exclamations']\n",
    "combined_df['total_stop_words'] = combined_df['stop_words_count'] + combined_df['num_exclamations']\n",
    "\n",
    "# Calculate the ratio of stop words to total words for each text\n",
    "combined_df['stop_words_ratio'] = combined_df['total_stop_words'] / combined_df['total_words']\n",
    "\n",
    "# Calculate the mean stop words ratio for true and fake news\n",
    "true_mean_ratio = combined_df[combined_df['Label']==True]['stop_words_ratio'].mean()\n",
    "fake_mean_ratio = combined_df[combined_df['Label']==False]['stop_words_ratio'].mean()\n",
    "\n",
    "print(f\"True news stop words ratio: {true_mean_ratio:.2%}\")\n",
    "print(f\"Fake news stop words ratio: {fake_mean_ratio:.2%}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This indicates that the number of stop words used in the text may not be a very strong indicator of whether a news article is true or fake. </br>\n",
    "Other factors may have a greater impact on determining the veracity of a news article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True news stop words ratio: 42.23%\n",
      "Fake news stop words ratio: 43.43%\n"
     ]
    }
   ],
   "source": [
    "# Calculate stop words ratio for true news\n",
    "true_df = combined_df[combined_df['Label'] == True]\n",
    "true_stop_words_count = true_df['stop_words_count'].sum()\n",
    "true_word_count = true_df['word_count'].sum()\n",
    "true_stop_words_ratio = true_stop_words_count / true_word_count\n",
    "print(f\"True news stop words ratio: {true_stop_words_ratio:.2%}\")\n",
    "\n",
    "# Calculate stop words ratio for fake news\n",
    "fake_df = combined_df[combined_df['Label'] == False]\n",
    "fake_stop_words_count = fake_df['stop_words_count'].sum()\n",
    "fake_word_count = fake_df['word_count'].sum()\n",
    "fake_stop_words_ratio = fake_stop_words_count / fake_word_count\n",
    "print(f\"Fake news stop words ratio: {fake_stop_words_ratio:.2%}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label\n",
      "False    442.507080\n",
      "True     543.208006\n",
      "Name: word_count, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Group the data by label and compute the average word count for each group\n",
    "avg_word_count = combined_df.groupby('Label')['word_count'].mean()\n",
    "\n",
    "# Print the results\n",
    "print(avg_word_count)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "looks like false news in this dataset has fewer average words compared to true news. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean normalized cap count for true news: 20.79%\n",
      "Mean normalized cap count for fake news: 28.63%\n"
     ]
    }
   ],
   "source": [
    "# Calculate the normalized cap count by dividing it with the word count\n",
    "normalized_cap_count = combined_df['cap_count'] / combined_df['word_count']\n",
    "\n",
    "# Create a boxplot to visualize the distribution of normalized cap count with respect to the label column\n",
    "sns.boxplot(x='Label', y=normalized_cap_count, data=combined_df)\n",
    "\n",
    "# Set the x-label and y-label\n",
    "plt.xlabel('Label')\n",
    "plt.ylabel('Normalized Cap Count')\n",
    "\n",
    "# Set the plot title\n",
    "plt.title('Normalized Cap Count vs Label')\n",
    "\n",
    "# Save the plot as a PNG file\n",
    "plt.savefig('normalized_cap_count_vs_label.png')\n",
    "\n",
    "# Print the mean normalized cap count for true and fake news\n",
    "print(\"Mean normalized cap count for true news: {:.2%}\".format(normalized_cap_count[combined_df['Label'] == True].mean()))\n",
    "print(\"Mean normalized cap count for fake news: {:.2%}\".format(normalized_cap_count[combined_df['Label'] == False].mean()))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "capital letters are more in fake news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean normalized punctuation count for true news: 12.57%\n",
      "Mean normalized punctuation count for fake news: 16.38%\n"
     ]
    }
   ],
   "source": [
    "# Calculate normalized punctuation count for each row\n",
    "combined_df['punct_count_norm'] = combined_df['punct_count'] / combined_df['word_count']\n",
    "\n",
    "# Calculate mean normalized punctuation count for true and fake news\n",
    "true_punct_mean = combined_df[combined_df['Label']==True]['punct_count_norm'].mean()\n",
    "fake_punct_mean = combined_df[combined_df['Label']==False]['punct_count_norm'].mean()\n",
    "\n",
    "print(f\"Mean normalized punctuation count for true news: {true_punct_mean:.2%}\")\n",
    "print(f\"Mean normalized punctuation count for fake news: {fake_punct_mean:.2%}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "punctuations are more in fake news. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean normalized number of exclamation marks for true news: 0.06%\n",
      "Mean normalized number of exclamation marks for fake news: 0.59%\n"
     ]
    }
   ],
   "source": [
    "# Calculate the normalized number of exclamation marks for each row\n",
    "combined_df['norm_num_exclamations'] = combined_df['num_exclamations'] / combined_df['word_count']\n",
    "\n",
    "# Calculate the mean normalized number of exclamation marks for each label\n",
    "mean_norm_excl_true = combined_df[combined_df['Label']==True]['norm_num_exclamations'].mean()\n",
    "mean_norm_excl_fake = combined_df[combined_df['Label']==False]['norm_num_exclamations'].mean()\n",
    "\n",
    "# Print the results\n",
    "print(\"Mean normalized number of exclamation marks for true news: {:.2%}\".format(mean_norm_excl_true))\n",
    "print(\"Mean normalized number of exclamation marks for fake news: {:.2%}\".format(mean_norm_excl_fake))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df = combined_df.drop('cap_words_headline', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>Label</th>\n",
       "      <th>stop_words_count</th>\n",
       "      <th>word_count</th>\n",
       "      <th>cap_count</th>\n",
       "      <th>punct_count</th>\n",
       "      <th>num_exclamations</th>\n",
       "      <th>cap_words_summary</th>\n",
       "      <th>total_words</th>\n",
       "      <th>total_stop_words</th>\n",
       "      <th>stop_words_ratio</th>\n",
       "      <th>punct_count_norm</th>\n",
       "      <th>norm_num_exclamations</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The head of a conservative Republican faction ...</td>\n",
       "      <td>True</td>\n",
       "      <td>303</td>\n",
       "      <td>749</td>\n",
       "      <td>144</td>\n",
       "      <td>115</td>\n",
       "      <td>0</td>\n",
       "      <td>144</td>\n",
       "      <td>749</td>\n",
       "      <td>303</td>\n",
       "      <td>0.404539</td>\n",
       "      <td>0.153538</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Transgender people will be allowed for the fir...</td>\n",
       "      <td>True</td>\n",
       "      <td>171</td>\n",
       "      <td>396</td>\n",
       "      <td>72</td>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "      <td>72</td>\n",
       "      <td>396</td>\n",
       "      <td>171</td>\n",
       "      <td>0.431818</td>\n",
       "      <td>0.101010</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The special counsel investigation of links bet...</td>\n",
       "      <td>True</td>\n",
       "      <td>201</td>\n",
       "      <td>462</td>\n",
       "      <td>106</td>\n",
       "      <td>44</td>\n",
       "      <td>0</td>\n",
       "      <td>106</td>\n",
       "      <td>462</td>\n",
       "      <td>201</td>\n",
       "      <td>0.435065</td>\n",
       "      <td>0.095238</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Trump campaign adviser George Papadopoulos tol...</td>\n",
       "      <td>True</td>\n",
       "      <td>153</td>\n",
       "      <td>373</td>\n",
       "      <td>99</td>\n",
       "      <td>48</td>\n",
       "      <td>0</td>\n",
       "      <td>99</td>\n",
       "      <td>373</td>\n",
       "      <td>153</td>\n",
       "      <td>0.410188</td>\n",
       "      <td>0.128686</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>President Donald Trump called on the U.S. Post...</td>\n",
       "      <td>True</td>\n",
       "      <td>348</td>\n",
       "      <td>860</td>\n",
       "      <td>182</td>\n",
       "      <td>132</td>\n",
       "      <td>2</td>\n",
       "      <td>182</td>\n",
       "      <td>862</td>\n",
       "      <td>350</td>\n",
       "      <td>0.406032</td>\n",
       "      <td>0.153488</td>\n",
       "      <td>0.002326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78612</th>\n",
       "      <td>The USA wants to divide Syria.\\r\\n\\r\\nGreat Br...</td>\n",
       "      <td>False</td>\n",
       "      <td>36</td>\n",
       "      <td>70</td>\n",
       "      <td>26</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>26</td>\n",
       "      <td>70</td>\n",
       "      <td>36</td>\n",
       "      <td>0.514286</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78613</th>\n",
       "      <td>The Ukrainian coup d'etat cost the US nothing ...</td>\n",
       "      <td>False</td>\n",
       "      <td>40</td>\n",
       "      <td>106</td>\n",
       "      <td>38</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>38</td>\n",
       "      <td>106</td>\n",
       "      <td>40</td>\n",
       "      <td>0.377358</td>\n",
       "      <td>0.122642</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78614</th>\n",
       "      <td>The European Parliament falsifies history by d...</td>\n",
       "      <td>False</td>\n",
       "      <td>61</td>\n",
       "      <td>129</td>\n",
       "      <td>28</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>28</td>\n",
       "      <td>129</td>\n",
       "      <td>61</td>\n",
       "      <td>0.472868</td>\n",
       "      <td>0.124031</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78615</th>\n",
       "      <td>The European Parliament falsifies history by d...</td>\n",
       "      <td>False</td>\n",
       "      <td>61</td>\n",
       "      <td>129</td>\n",
       "      <td>28</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>28</td>\n",
       "      <td>129</td>\n",
       "      <td>61</td>\n",
       "      <td>0.472868</td>\n",
       "      <td>0.124031</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78616</th>\n",
       "      <td>A leading FSB officer, Segey Beseda, said duri...</td>\n",
       "      <td>False</td>\n",
       "      <td>24</td>\n",
       "      <td>55</td>\n",
       "      <td>26</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>26</td>\n",
       "      <td>55</td>\n",
       "      <td>24</td>\n",
       "      <td>0.436364</td>\n",
       "      <td>0.109091</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>78617 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  Label  \\\n",
       "0      The head of a conservative Republican faction ...   True   \n",
       "1      Transgender people will be allowed for the fir...   True   \n",
       "2      The special counsel investigation of links bet...   True   \n",
       "3      Trump campaign adviser George Papadopoulos tol...   True   \n",
       "4      President Donald Trump called on the U.S. Post...   True   \n",
       "...                                                  ...    ...   \n",
       "78612  The USA wants to divide Syria.\\r\\n\\r\\nGreat Br...  False   \n",
       "78613  The Ukrainian coup d'etat cost the US nothing ...  False   \n",
       "78614  The European Parliament falsifies history by d...  False   \n",
       "78615  The European Parliament falsifies history by d...  False   \n",
       "78616  A leading FSB officer, Segey Beseda, said duri...  False   \n",
       "\n",
       "       stop_words_count  word_count  cap_count  punct_count  num_exclamations  \\\n",
       "0                   303         749        144          115                 0   \n",
       "1                   171         396         72           40                 0   \n",
       "2                   201         462        106           44                 0   \n",
       "3                   153         373         99           48                 0   \n",
       "4                   348         860        182          132                 2   \n",
       "...                 ...         ...        ...          ...               ...   \n",
       "78612                36          70         26           14                 0   \n",
       "78613                40         106         38           13                 0   \n",
       "78614                61         129         28           16                 0   \n",
       "78615                61         129         28           16                 0   \n",
       "78616                24          55         26            6                 0   \n",
       "\n",
       "       cap_words_summary  total_words  total_stop_words  stop_words_ratio  \\\n",
       "0                    144          749               303          0.404539   \n",
       "1                     72          396               171          0.431818   \n",
       "2                    106          462               201          0.435065   \n",
       "3                     99          373               153          0.410188   \n",
       "4                    182          862               350          0.406032   \n",
       "...                  ...          ...               ...               ...   \n",
       "78612                 26           70                36          0.514286   \n",
       "78613                 38          106                40          0.377358   \n",
       "78614                 28          129                61          0.472868   \n",
       "78615                 28          129                61          0.472868   \n",
       "78616                 26           55                24          0.436364   \n",
       "\n",
       "       punct_count_norm  norm_num_exclamations  \n",
       "0              0.153538               0.000000  \n",
       "1              0.101010               0.000000  \n",
       "2              0.095238               0.000000  \n",
       "3              0.128686               0.000000  \n",
       "4              0.153488               0.002326  \n",
       "...                 ...                    ...  \n",
       "78612          0.200000               0.000000  \n",
       "78613          0.122642               0.000000  \n",
       "78614          0.124031               0.000000  \n",
       "78615          0.124031               0.000000  \n",
       "78616          0.109091               0.000000  \n",
       "\n",
       "[78617 rows x 13 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: vaderSentiment in /home/theprofessional/anaconda3/lib/python3.10/site-packages (3.3.2)\n",
      "Requirement already satisfied: requests in /home/theprofessional/anaconda3/lib/python3.10/site-packages (from vaderSentiment) (2.28.1)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /home/theprofessional/anaconda3/lib/python3.10/site-packages (from requests->vaderSentiment) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/theprofessional/anaconda3/lib/python3.10/site-packages (from requests->vaderSentiment) (2022.12.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/theprofessional/anaconda3/lib/python3.10/site-packages (from requests->vaderSentiment) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/theprofessional/anaconda3/lib/python3.10/site-packages (from requests->vaderSentiment) (1.26.14)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install vaderSentiment"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "use the VADER (Valence Aware Dictionary and Sentiment Reasoner) sentiment analysis tool to analyze the sentiment of the text in the text column.\n",
    "\n",
    "First, we need to install the vaderSentiment library which contains the VADER sentiment analysis tool. We can do this by running !pip install vaderSentiment in a code cell.\n",
    "\n",
    "Then we can import the SentimentIntensityAnalyzer class from the vaderSentiment library and create an instance of the class to use for sentiment analysis.\n",
    "\n",
    "After that, we can define a function that takes a piece of text as input, uses the SentimentIntensityAnalyzer to analyze the sentiment of the text, and returns the sentiment scores as a dictionary.\n",
    "\n",
    "Finally, we can apply this function to the text column of the dataframe using the apply() method and create a new column in the dataframe to store the sentiment scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "# Create an instance of the SentimentIntensityAnalyzer class\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Define a function to analyze the sentiment of a piece of text\n",
    "def analyze_sentiment(text):\n",
    "    scores = analyzer.polarity_scores(text)\n",
    "    return scores\n",
    "\n",
    "# Apply the analyze_sentiment function to the text column of the dataframe and create a new column to store the sentiment scores\n",
    "combined_df['sentiment_scores'] = combined_df['text'].apply(analyze_sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>Label</th>\n",
       "      <th>stop_words_count</th>\n",
       "      <th>word_count</th>\n",
       "      <th>cap_count</th>\n",
       "      <th>punct_count</th>\n",
       "      <th>num_exclamations</th>\n",
       "      <th>cap_words_summary</th>\n",
       "      <th>total_words</th>\n",
       "      <th>total_stop_words</th>\n",
       "      <th>stop_words_ratio</th>\n",
       "      <th>punct_count_norm</th>\n",
       "      <th>norm_num_exclamations</th>\n",
       "      <th>sentiment_scores</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The head of a conservative Republican faction ...</td>\n",
       "      <td>True</td>\n",
       "      <td>303</td>\n",
       "      <td>749</td>\n",
       "      <td>144</td>\n",
       "      <td>115</td>\n",
       "      <td>0</td>\n",
       "      <td>144</td>\n",
       "      <td>749</td>\n",
       "      <td>303</td>\n",
       "      <td>0.404539</td>\n",
       "      <td>0.153538</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>{'neg': 0.048, 'neu': 0.866, 'pos': 0.086, 'co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Transgender people will be allowed for the fir...</td>\n",
       "      <td>True</td>\n",
       "      <td>171</td>\n",
       "      <td>396</td>\n",
       "      <td>72</td>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "      <td>72</td>\n",
       "      <td>396</td>\n",
       "      <td>171</td>\n",
       "      <td>0.431818</td>\n",
       "      <td>0.101010</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>{'neg': 0.063, 'neu': 0.837, 'pos': 0.101, 'co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The special counsel investigation of links bet...</td>\n",
       "      <td>True</td>\n",
       "      <td>201</td>\n",
       "      <td>462</td>\n",
       "      <td>106</td>\n",
       "      <td>44</td>\n",
       "      <td>0</td>\n",
       "      <td>106</td>\n",
       "      <td>462</td>\n",
       "      <td>201</td>\n",
       "      <td>0.435065</td>\n",
       "      <td>0.095238</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>{'neg': 0.049, 'neu': 0.907, 'pos': 0.044, 'co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Trump campaign adviser George Papadopoulos tol...</td>\n",
       "      <td>True</td>\n",
       "      <td>153</td>\n",
       "      <td>373</td>\n",
       "      <td>99</td>\n",
       "      <td>48</td>\n",
       "      <td>0</td>\n",
       "      <td>99</td>\n",
       "      <td>373</td>\n",
       "      <td>153</td>\n",
       "      <td>0.410188</td>\n",
       "      <td>0.128686</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>{'neg': 0.066, 'neu': 0.875, 'pos': 0.059, 'co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>President Donald Trump called on the U.S. Post...</td>\n",
       "      <td>True</td>\n",
       "      <td>348</td>\n",
       "      <td>860</td>\n",
       "      <td>182</td>\n",
       "      <td>132</td>\n",
       "      <td>2</td>\n",
       "      <td>182</td>\n",
       "      <td>862</td>\n",
       "      <td>350</td>\n",
       "      <td>0.406032</td>\n",
       "      <td>0.153488</td>\n",
       "      <td>0.002326</td>\n",
       "      <td>{'neg': 0.043, 'neu': 0.898, 'pos': 0.059, 'co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78612</th>\n",
       "      <td>The USA wants to divide Syria.\\r\\n\\r\\nGreat Br...</td>\n",
       "      <td>False</td>\n",
       "      <td>36</td>\n",
       "      <td>70</td>\n",
       "      <td>26</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>26</td>\n",
       "      <td>70</td>\n",
       "      <td>36</td>\n",
       "      <td>0.514286</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>{'neg': 0.0, 'neu': 0.901, 'pos': 0.099, 'comp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78613</th>\n",
       "      <td>The Ukrainian coup d'etat cost the US nothing ...</td>\n",
       "      <td>False</td>\n",
       "      <td>40</td>\n",
       "      <td>106</td>\n",
       "      <td>38</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>38</td>\n",
       "      <td>106</td>\n",
       "      <td>40</td>\n",
       "      <td>0.377358</td>\n",
       "      <td>0.122642</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>{'neg': 0.082, 'neu': 0.891, 'pos': 0.026, 'co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78614</th>\n",
       "      <td>The European Parliament falsifies history by d...</td>\n",
       "      <td>False</td>\n",
       "      <td>61</td>\n",
       "      <td>129</td>\n",
       "      <td>28</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>28</td>\n",
       "      <td>129</td>\n",
       "      <td>61</td>\n",
       "      <td>0.472868</td>\n",
       "      <td>0.124031</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>{'neg': 0.146, 'neu': 0.796, 'pos': 0.058, 'co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78615</th>\n",
       "      <td>The European Parliament falsifies history by d...</td>\n",
       "      <td>False</td>\n",
       "      <td>61</td>\n",
       "      <td>129</td>\n",
       "      <td>28</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>28</td>\n",
       "      <td>129</td>\n",
       "      <td>61</td>\n",
       "      <td>0.472868</td>\n",
       "      <td>0.124031</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>{'neg': 0.146, 'neu': 0.796, 'pos': 0.058, 'co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78616</th>\n",
       "      <td>A leading FSB officer, Segey Beseda, said duri...</td>\n",
       "      <td>False</td>\n",
       "      <td>24</td>\n",
       "      <td>55</td>\n",
       "      <td>26</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>26</td>\n",
       "      <td>55</td>\n",
       "      <td>24</td>\n",
       "      <td>0.436364</td>\n",
       "      <td>0.109091</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>{'neg': 0.066, 'neu': 0.835, 'pos': 0.1, 'comp...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>78617 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  Label  \\\n",
       "0      The head of a conservative Republican faction ...   True   \n",
       "1      Transgender people will be allowed for the fir...   True   \n",
       "2      The special counsel investigation of links bet...   True   \n",
       "3      Trump campaign adviser George Papadopoulos tol...   True   \n",
       "4      President Donald Trump called on the U.S. Post...   True   \n",
       "...                                                  ...    ...   \n",
       "78612  The USA wants to divide Syria.\\r\\n\\r\\nGreat Br...  False   \n",
       "78613  The Ukrainian coup d'etat cost the US nothing ...  False   \n",
       "78614  The European Parliament falsifies history by d...  False   \n",
       "78615  The European Parliament falsifies history by d...  False   \n",
       "78616  A leading FSB officer, Segey Beseda, said duri...  False   \n",
       "\n",
       "       stop_words_count  word_count  cap_count  punct_count  num_exclamations  \\\n",
       "0                   303         749        144          115                 0   \n",
       "1                   171         396         72           40                 0   \n",
       "2                   201         462        106           44                 0   \n",
       "3                   153         373         99           48                 0   \n",
       "4                   348         860        182          132                 2   \n",
       "...                 ...         ...        ...          ...               ...   \n",
       "78612                36          70         26           14                 0   \n",
       "78613                40         106         38           13                 0   \n",
       "78614                61         129         28           16                 0   \n",
       "78615                61         129         28           16                 0   \n",
       "78616                24          55         26            6                 0   \n",
       "\n",
       "       cap_words_summary  total_words  total_stop_words  stop_words_ratio  \\\n",
       "0                    144          749               303          0.404539   \n",
       "1                     72          396               171          0.431818   \n",
       "2                    106          462               201          0.435065   \n",
       "3                     99          373               153          0.410188   \n",
       "4                    182          862               350          0.406032   \n",
       "...                  ...          ...               ...               ...   \n",
       "78612                 26           70                36          0.514286   \n",
       "78613                 38          106                40          0.377358   \n",
       "78614                 28          129                61          0.472868   \n",
       "78615                 28          129                61          0.472868   \n",
       "78616                 26           55                24          0.436364   \n",
       "\n",
       "       punct_count_norm  norm_num_exclamations  \\\n",
       "0              0.153538               0.000000   \n",
       "1              0.101010               0.000000   \n",
       "2              0.095238               0.000000   \n",
       "3              0.128686               0.000000   \n",
       "4              0.153488               0.002326   \n",
       "...                 ...                    ...   \n",
       "78612          0.200000               0.000000   \n",
       "78613          0.122642               0.000000   \n",
       "78614          0.124031               0.000000   \n",
       "78615          0.124031               0.000000   \n",
       "78616          0.109091               0.000000   \n",
       "\n",
       "                                        sentiment_scores  \n",
       "0      {'neg': 0.048, 'neu': 0.866, 'pos': 0.086, 'co...  \n",
       "1      {'neg': 0.063, 'neu': 0.837, 'pos': 0.101, 'co...  \n",
       "2      {'neg': 0.049, 'neu': 0.907, 'pos': 0.044, 'co...  \n",
       "3      {'neg': 0.066, 'neu': 0.875, 'pos': 0.059, 'co...  \n",
       "4      {'neg': 0.043, 'neu': 0.898, 'pos': 0.059, 'co...  \n",
       "...                                                  ...  \n",
       "78612  {'neg': 0.0, 'neu': 0.901, 'pos': 0.099, 'comp...  \n",
       "78613  {'neg': 0.082, 'neu': 0.891, 'pos': 0.026, 'co...  \n",
       "78614  {'neg': 0.146, 'neu': 0.796, 'pos': 0.058, 'co...  \n",
       "78615  {'neg': 0.146, 'neu': 0.796, 'pos': 0.058, 'co...  \n",
       "78616  {'neg': 0.066, 'neu': 0.835, 'pos': 0.1, 'comp...  \n",
       "\n",
       "[78617 rows x 14 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new column 'sentiment_scores_num' with numerical sentiment scores\n",
    "combined_df['sentiment_scores_num'] = combined_df['sentiment_scores'].apply(lambda x: x['compound'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Separate the data based on label\n",
    "true_df = combined_df[combined_df['Label']]\n",
    "false_df = combined_df[~combined_df['Label']]\n",
    "\n",
    "# Create a new column 'sentiment_scores_num' with numerical sentiment scores\n",
    "combined_df['sentiment_scores_num'] = combined_df['sentiment_scores'].apply(lambda x: x['compound'])\n",
    "\n",
    "# Plot histograms of sentiment scores for each label\n",
    "plt.hist(true_df['sentiment_scores_num'], bins=20, alpha=0.5, label='True')\n",
    "plt.hist(false_df['sentiment_scores_num'], bins=20, alpha=0.5, label='False')\n",
    "plt.legend(loc='upper left')\n",
    "plt.xlabel('Sentiment Score')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Sentiment Scores by Label')\n",
    "plt.savefig('sentiment_histogram.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>Label</th>\n",
       "      <th>stop_words_count</th>\n",
       "      <th>word_count</th>\n",
       "      <th>cap_count</th>\n",
       "      <th>punct_count</th>\n",
       "      <th>num_exclamations</th>\n",
       "      <th>cap_words_summary</th>\n",
       "      <th>total_words</th>\n",
       "      <th>total_stop_words</th>\n",
       "      <th>stop_words_ratio</th>\n",
       "      <th>punct_count_norm</th>\n",
       "      <th>norm_num_exclamations</th>\n",
       "      <th>sentiment_scores</th>\n",
       "      <th>sentiment_scores_num</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The head of a conservative Republican faction ...</td>\n",
       "      <td>True</td>\n",
       "      <td>303</td>\n",
       "      <td>749</td>\n",
       "      <td>144</td>\n",
       "      <td>115</td>\n",
       "      <td>0</td>\n",
       "      <td>144</td>\n",
       "      <td>749</td>\n",
       "      <td>303</td>\n",
       "      <td>0.404539</td>\n",
       "      <td>0.153538</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>{'neg': 0.048, 'neu': 0.866, 'pos': 0.086, 'co...</td>\n",
       "      <td>0.9847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Transgender people will be allowed for the fir...</td>\n",
       "      <td>True</td>\n",
       "      <td>171</td>\n",
       "      <td>396</td>\n",
       "      <td>72</td>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "      <td>72</td>\n",
       "      <td>396</td>\n",
       "      <td>171</td>\n",
       "      <td>0.431818</td>\n",
       "      <td>0.101010</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>{'neg': 0.063, 'neu': 0.837, 'pos': 0.101, 'co...</td>\n",
       "      <td>0.9496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The special counsel investigation of links bet...</td>\n",
       "      <td>True</td>\n",
       "      <td>201</td>\n",
       "      <td>462</td>\n",
       "      <td>106</td>\n",
       "      <td>44</td>\n",
       "      <td>0</td>\n",
       "      <td>106</td>\n",
       "      <td>462</td>\n",
       "      <td>201</td>\n",
       "      <td>0.435065</td>\n",
       "      <td>0.095238</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>{'neg': 0.049, 'neu': 0.907, 'pos': 0.044, 'co...</td>\n",
       "      <td>-0.6808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Trump campaign adviser George Papadopoulos tol...</td>\n",
       "      <td>True</td>\n",
       "      <td>153</td>\n",
       "      <td>373</td>\n",
       "      <td>99</td>\n",
       "      <td>48</td>\n",
       "      <td>0</td>\n",
       "      <td>99</td>\n",
       "      <td>373</td>\n",
       "      <td>153</td>\n",
       "      <td>0.410188</td>\n",
       "      <td>0.128686</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>{'neg': 0.066, 'neu': 0.875, 'pos': 0.059, 'co...</td>\n",
       "      <td>-0.2201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>President Donald Trump called on the U.S. Post...</td>\n",
       "      <td>True</td>\n",
       "      <td>348</td>\n",
       "      <td>860</td>\n",
       "      <td>182</td>\n",
       "      <td>132</td>\n",
       "      <td>2</td>\n",
       "      <td>182</td>\n",
       "      <td>862</td>\n",
       "      <td>350</td>\n",
       "      <td>0.406032</td>\n",
       "      <td>0.153488</td>\n",
       "      <td>0.002326</td>\n",
       "      <td>{'neg': 0.043, 'neu': 0.898, 'pos': 0.059, 'co...</td>\n",
       "      <td>0.6683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78612</th>\n",
       "      <td>The USA wants to divide Syria.\\r\\n\\r\\nGreat Br...</td>\n",
       "      <td>False</td>\n",
       "      <td>36</td>\n",
       "      <td>70</td>\n",
       "      <td>26</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>26</td>\n",
       "      <td>70</td>\n",
       "      <td>36</td>\n",
       "      <td>0.514286</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>{'neg': 0.0, 'neu': 0.901, 'pos': 0.099, 'comp...</td>\n",
       "      <td>0.7650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78613</th>\n",
       "      <td>The Ukrainian coup d'etat cost the US nothing ...</td>\n",
       "      <td>False</td>\n",
       "      <td>40</td>\n",
       "      <td>106</td>\n",
       "      <td>38</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>38</td>\n",
       "      <td>106</td>\n",
       "      <td>40</td>\n",
       "      <td>0.377358</td>\n",
       "      <td>0.122642</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>{'neg': 0.082, 'neu': 0.891, 'pos': 0.026, 'co...</td>\n",
       "      <td>-0.6492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78614</th>\n",
       "      <td>The European Parliament falsifies history by d...</td>\n",
       "      <td>False</td>\n",
       "      <td>61</td>\n",
       "      <td>129</td>\n",
       "      <td>28</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>28</td>\n",
       "      <td>129</td>\n",
       "      <td>61</td>\n",
       "      <td>0.472868</td>\n",
       "      <td>0.124031</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>{'neg': 0.146, 'neu': 0.796, 'pos': 0.058, 'co...</td>\n",
       "      <td>-0.9217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78615</th>\n",
       "      <td>The European Parliament falsifies history by d...</td>\n",
       "      <td>False</td>\n",
       "      <td>61</td>\n",
       "      <td>129</td>\n",
       "      <td>28</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>28</td>\n",
       "      <td>129</td>\n",
       "      <td>61</td>\n",
       "      <td>0.472868</td>\n",
       "      <td>0.124031</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>{'neg': 0.146, 'neu': 0.796, 'pos': 0.058, 'co...</td>\n",
       "      <td>-0.9217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78616</th>\n",
       "      <td>A leading FSB officer, Segey Beseda, said duri...</td>\n",
       "      <td>False</td>\n",
       "      <td>24</td>\n",
       "      <td>55</td>\n",
       "      <td>26</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>26</td>\n",
       "      <td>55</td>\n",
       "      <td>24</td>\n",
       "      <td>0.436364</td>\n",
       "      <td>0.109091</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>{'neg': 0.066, 'neu': 0.835, 'pos': 0.1, 'comp...</td>\n",
       "      <td>0.2732</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>78617 rows × 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  Label  \\\n",
       "0      The head of a conservative Republican faction ...   True   \n",
       "1      Transgender people will be allowed for the fir...   True   \n",
       "2      The special counsel investigation of links bet...   True   \n",
       "3      Trump campaign adviser George Papadopoulos tol...   True   \n",
       "4      President Donald Trump called on the U.S. Post...   True   \n",
       "...                                                  ...    ...   \n",
       "78612  The USA wants to divide Syria.\\r\\n\\r\\nGreat Br...  False   \n",
       "78613  The Ukrainian coup d'etat cost the US nothing ...  False   \n",
       "78614  The European Parliament falsifies history by d...  False   \n",
       "78615  The European Parliament falsifies history by d...  False   \n",
       "78616  A leading FSB officer, Segey Beseda, said duri...  False   \n",
       "\n",
       "       stop_words_count  word_count  cap_count  punct_count  num_exclamations  \\\n",
       "0                   303         749        144          115                 0   \n",
       "1                   171         396         72           40                 0   \n",
       "2                   201         462        106           44                 0   \n",
       "3                   153         373         99           48                 0   \n",
       "4                   348         860        182          132                 2   \n",
       "...                 ...         ...        ...          ...               ...   \n",
       "78612                36          70         26           14                 0   \n",
       "78613                40         106         38           13                 0   \n",
       "78614                61         129         28           16                 0   \n",
       "78615                61         129         28           16                 0   \n",
       "78616                24          55         26            6                 0   \n",
       "\n",
       "       cap_words_summary  total_words  total_stop_words  stop_words_ratio  \\\n",
       "0                    144          749               303          0.404539   \n",
       "1                     72          396               171          0.431818   \n",
       "2                    106          462               201          0.435065   \n",
       "3                     99          373               153          0.410188   \n",
       "4                    182          862               350          0.406032   \n",
       "...                  ...          ...               ...               ...   \n",
       "78612                 26           70                36          0.514286   \n",
       "78613                 38          106                40          0.377358   \n",
       "78614                 28          129                61          0.472868   \n",
       "78615                 28          129                61          0.472868   \n",
       "78616                 26           55                24          0.436364   \n",
       "\n",
       "       punct_count_norm  norm_num_exclamations  \\\n",
       "0              0.153538               0.000000   \n",
       "1              0.101010               0.000000   \n",
       "2              0.095238               0.000000   \n",
       "3              0.128686               0.000000   \n",
       "4              0.153488               0.002326   \n",
       "...                 ...                    ...   \n",
       "78612          0.200000               0.000000   \n",
       "78613          0.122642               0.000000   \n",
       "78614          0.124031               0.000000   \n",
       "78615          0.124031               0.000000   \n",
       "78616          0.109091               0.000000   \n",
       "\n",
       "                                        sentiment_scores  sentiment_scores_num  \n",
       "0      {'neg': 0.048, 'neu': 0.866, 'pos': 0.086, 'co...                0.9847  \n",
       "1      {'neg': 0.063, 'neu': 0.837, 'pos': 0.101, 'co...                0.9496  \n",
       "2      {'neg': 0.049, 'neu': 0.907, 'pos': 0.044, 'co...               -0.6808  \n",
       "3      {'neg': 0.066, 'neu': 0.875, 'pos': 0.059, 'co...               -0.2201  \n",
       "4      {'neg': 0.043, 'neu': 0.898, 'pos': 0.059, 'co...                0.6683  \n",
       "...                                                  ...                   ...  \n",
       "78612  {'neg': 0.0, 'neu': 0.901, 'pos': 0.099, 'comp...                0.7650  \n",
       "78613  {'neg': 0.082, 'neu': 0.891, 'pos': 0.026, 'co...               -0.6492  \n",
       "78614  {'neg': 0.146, 'neu': 0.796, 'pos': 0.058, 'co...               -0.9217  \n",
       "78615  {'neg': 0.146, 'neu': 0.796, 'pos': 0.058, 'co...               -0.9217  \n",
       "78616  {'neg': 0.066, 'neu': 0.835, 'pos': 0.1, 'comp...                0.2732  \n",
       "\n",
       "[78617 rows x 15 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here this is important. True news is more positive in this dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Separate the data based on label\n",
    "true_df = combined_df[combined_df['Label']]\n",
    "false_df = combined_df[~combined_df['Label']]\n",
    "\n",
    "# Plot box plots of sentiment scores for each label\n",
    "fig, ax = plt.subplots()\n",
    "ax.boxplot([true_df['sentiment_scores_num'], false_df['sentiment_scores_num']])\n",
    "ax.set_xticklabels(['True', 'False'])\n",
    "ax.set_ylabel('Sentiment Score')\n",
    "ax.set_title('Distribution of Sentiment Scores by Label')\n",
    "plt.savefig('sentiment_boxplot.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Tokenize the text\n",
    "    tokens = word_tokenize(text.lower())\n",
    "\n",
    "    # Remove stopwords and non-alphabetic characters\n",
    "    tokens = [token for token in tokens if token.isalpha() and token not in stop_words]\n",
    "\n",
    "    # Lemmatize the tokens\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>Label</th>\n",
       "      <th>stop_words_count</th>\n",
       "      <th>word_count</th>\n",
       "      <th>cap_count</th>\n",
       "      <th>punct_count</th>\n",
       "      <th>num_exclamations</th>\n",
       "      <th>cap_words_summary</th>\n",
       "      <th>total_words</th>\n",
       "      <th>total_stop_words</th>\n",
       "      <th>stop_words_ratio</th>\n",
       "      <th>punct_count_norm</th>\n",
       "      <th>norm_num_exclamations</th>\n",
       "      <th>sentiment_scores</th>\n",
       "      <th>sentiment_scores_num</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The head of a conservative Republican faction ...</td>\n",
       "      <td>True</td>\n",
       "      <td>303</td>\n",
       "      <td>749</td>\n",
       "      <td>144</td>\n",
       "      <td>115</td>\n",
       "      <td>0</td>\n",
       "      <td>144</td>\n",
       "      <td>749</td>\n",
       "      <td>303</td>\n",
       "      <td>0.404539</td>\n",
       "      <td>0.153538</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>{'neg': 0.048, 'neu': 0.866, 'pos': 0.086, 'co...</td>\n",
       "      <td>0.9847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Transgender people will be allowed for the fir...</td>\n",
       "      <td>True</td>\n",
       "      <td>171</td>\n",
       "      <td>396</td>\n",
       "      <td>72</td>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "      <td>72</td>\n",
       "      <td>396</td>\n",
       "      <td>171</td>\n",
       "      <td>0.431818</td>\n",
       "      <td>0.101010</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>{'neg': 0.063, 'neu': 0.837, 'pos': 0.101, 'co...</td>\n",
       "      <td>0.9496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The special counsel investigation of links bet...</td>\n",
       "      <td>True</td>\n",
       "      <td>201</td>\n",
       "      <td>462</td>\n",
       "      <td>106</td>\n",
       "      <td>44</td>\n",
       "      <td>0</td>\n",
       "      <td>106</td>\n",
       "      <td>462</td>\n",
       "      <td>201</td>\n",
       "      <td>0.435065</td>\n",
       "      <td>0.095238</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>{'neg': 0.049, 'neu': 0.907, 'pos': 0.044, 'co...</td>\n",
       "      <td>-0.6808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Trump campaign adviser George Papadopoulos tol...</td>\n",
       "      <td>True</td>\n",
       "      <td>153</td>\n",
       "      <td>373</td>\n",
       "      <td>99</td>\n",
       "      <td>48</td>\n",
       "      <td>0</td>\n",
       "      <td>99</td>\n",
       "      <td>373</td>\n",
       "      <td>153</td>\n",
       "      <td>0.410188</td>\n",
       "      <td>0.128686</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>{'neg': 0.066, 'neu': 0.875, 'pos': 0.059, 'co...</td>\n",
       "      <td>-0.2201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>President Donald Trump called on the U.S. Post...</td>\n",
       "      <td>True</td>\n",
       "      <td>348</td>\n",
       "      <td>860</td>\n",
       "      <td>182</td>\n",
       "      <td>132</td>\n",
       "      <td>2</td>\n",
       "      <td>182</td>\n",
       "      <td>862</td>\n",
       "      <td>350</td>\n",
       "      <td>0.406032</td>\n",
       "      <td>0.153488</td>\n",
       "      <td>0.002326</td>\n",
       "      <td>{'neg': 0.043, 'neu': 0.898, 'pos': 0.059, 'co...</td>\n",
       "      <td>0.6683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78612</th>\n",
       "      <td>The USA wants to divide Syria.\\r\\n\\r\\nGreat Br...</td>\n",
       "      <td>False</td>\n",
       "      <td>36</td>\n",
       "      <td>70</td>\n",
       "      <td>26</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>26</td>\n",
       "      <td>70</td>\n",
       "      <td>36</td>\n",
       "      <td>0.514286</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>{'neg': 0.0, 'neu': 0.901, 'pos': 0.099, 'comp...</td>\n",
       "      <td>0.7650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78613</th>\n",
       "      <td>The Ukrainian coup d'etat cost the US nothing ...</td>\n",
       "      <td>False</td>\n",
       "      <td>40</td>\n",
       "      <td>106</td>\n",
       "      <td>38</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>38</td>\n",
       "      <td>106</td>\n",
       "      <td>40</td>\n",
       "      <td>0.377358</td>\n",
       "      <td>0.122642</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>{'neg': 0.082, 'neu': 0.891, 'pos': 0.026, 'co...</td>\n",
       "      <td>-0.6492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78614</th>\n",
       "      <td>The European Parliament falsifies history by d...</td>\n",
       "      <td>False</td>\n",
       "      <td>61</td>\n",
       "      <td>129</td>\n",
       "      <td>28</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>28</td>\n",
       "      <td>129</td>\n",
       "      <td>61</td>\n",
       "      <td>0.472868</td>\n",
       "      <td>0.124031</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>{'neg': 0.146, 'neu': 0.796, 'pos': 0.058, 'co...</td>\n",
       "      <td>-0.9217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78615</th>\n",
       "      <td>The European Parliament falsifies history by d...</td>\n",
       "      <td>False</td>\n",
       "      <td>61</td>\n",
       "      <td>129</td>\n",
       "      <td>28</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>28</td>\n",
       "      <td>129</td>\n",
       "      <td>61</td>\n",
       "      <td>0.472868</td>\n",
       "      <td>0.124031</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>{'neg': 0.146, 'neu': 0.796, 'pos': 0.058, 'co...</td>\n",
       "      <td>-0.9217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78616</th>\n",
       "      <td>A leading FSB officer, Segey Beseda, said duri...</td>\n",
       "      <td>False</td>\n",
       "      <td>24</td>\n",
       "      <td>55</td>\n",
       "      <td>26</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>26</td>\n",
       "      <td>55</td>\n",
       "      <td>24</td>\n",
       "      <td>0.436364</td>\n",
       "      <td>0.109091</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>{'neg': 0.066, 'neu': 0.835, 'pos': 0.1, 'comp...</td>\n",
       "      <td>0.2732</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>78617 rows × 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  Label  \\\n",
       "0      The head of a conservative Republican faction ...   True   \n",
       "1      Transgender people will be allowed for the fir...   True   \n",
       "2      The special counsel investigation of links bet...   True   \n",
       "3      Trump campaign adviser George Papadopoulos tol...   True   \n",
       "4      President Donald Trump called on the U.S. Post...   True   \n",
       "...                                                  ...    ...   \n",
       "78612  The USA wants to divide Syria.\\r\\n\\r\\nGreat Br...  False   \n",
       "78613  The Ukrainian coup d'etat cost the US nothing ...  False   \n",
       "78614  The European Parliament falsifies history by d...  False   \n",
       "78615  The European Parliament falsifies history by d...  False   \n",
       "78616  A leading FSB officer, Segey Beseda, said duri...  False   \n",
       "\n",
       "       stop_words_count  word_count  cap_count  punct_count  num_exclamations  \\\n",
       "0                   303         749        144          115                 0   \n",
       "1                   171         396         72           40                 0   \n",
       "2                   201         462        106           44                 0   \n",
       "3                   153         373         99           48                 0   \n",
       "4                   348         860        182          132                 2   \n",
       "...                 ...         ...        ...          ...               ...   \n",
       "78612                36          70         26           14                 0   \n",
       "78613                40         106         38           13                 0   \n",
       "78614                61         129         28           16                 0   \n",
       "78615                61         129         28           16                 0   \n",
       "78616                24          55         26            6                 0   \n",
       "\n",
       "       cap_words_summary  total_words  total_stop_words  stop_words_ratio  \\\n",
       "0                    144          749               303          0.404539   \n",
       "1                     72          396               171          0.431818   \n",
       "2                    106          462               201          0.435065   \n",
       "3                     99          373               153          0.410188   \n",
       "4                    182          862               350          0.406032   \n",
       "...                  ...          ...               ...               ...   \n",
       "78612                 26           70                36          0.514286   \n",
       "78613                 38          106                40          0.377358   \n",
       "78614                 28          129                61          0.472868   \n",
       "78615                 28          129                61          0.472868   \n",
       "78616                 26           55                24          0.436364   \n",
       "\n",
       "       punct_count_norm  norm_num_exclamations  \\\n",
       "0              0.153538               0.000000   \n",
       "1              0.101010               0.000000   \n",
       "2              0.095238               0.000000   \n",
       "3              0.128686               0.000000   \n",
       "4              0.153488               0.002326   \n",
       "...                 ...                    ...   \n",
       "78612          0.200000               0.000000   \n",
       "78613          0.122642               0.000000   \n",
       "78614          0.124031               0.000000   \n",
       "78615          0.124031               0.000000   \n",
       "78616          0.109091               0.000000   \n",
       "\n",
       "                                        sentiment_scores  sentiment_scores_num  \n",
       "0      {'neg': 0.048, 'neu': 0.866, 'pos': 0.086, 'co...                0.9847  \n",
       "1      {'neg': 0.063, 'neu': 0.837, 'pos': 0.101, 'co...                0.9496  \n",
       "2      {'neg': 0.049, 'neu': 0.907, 'pos': 0.044, 'co...               -0.6808  \n",
       "3      {'neg': 0.066, 'neu': 0.875, 'pos': 0.059, 'co...               -0.2201  \n",
       "4      {'neg': 0.043, 'neu': 0.898, 'pos': 0.059, 'co...                0.6683  \n",
       "...                                                  ...                   ...  \n",
       "78612  {'neg': 0.0, 'neu': 0.901, 'pos': 0.099, 'comp...                0.7650  \n",
       "78613  {'neg': 0.082, 'neu': 0.891, 'pos': 0.026, 'co...               -0.6492  \n",
       "78614  {'neg': 0.146, 'neu': 0.796, 'pos': 0.058, 'co...               -0.9217  \n",
       "78615  {'neg': 0.146, 'neu': 0.796, 'pos': 0.058, 'co...               -0.9217  \n",
       "78616  {'neg': 0.066, 'neu': 0.835, 'pos': 0.1, 'comp...                0.2732  \n",
       "\n",
       "[78617 rows x 15 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Okay I neeed to clear my mind here. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# separate data into true and false news\n",
    "true_df = combined_df[combined_df['Label'] == True]\n",
    "false_df = combined_df[combined_df['Label'] == False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def t_test(true_data, fake_data):\n",
    "    t_values = {}\n",
    "    p_values = {}\n",
    "    for col in true_data.columns:\n",
    "        t, p = stats.ttest_ind(true_data[col], fake_data[col], equal_var=False, nan_policy='omit')\n",
    "        t_values[col] = t\n",
    "        p_values[col] = p\n",
    "    return t_values, p_values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stop_words_count\n",
      "T-test: t = 18.7199  p-value = 5.05826e-78\n",
      "word_count\n",
      "T-test: t = 22.9724  p-value = 2.15588e-116\n",
      "cap_count\n",
      "T-test: t = 0.843846  p-value = 0.398758\n",
      "punct_count\n",
      "T-test: t = 14.7224  p-value = 5.38372e-49\n",
      "num_exclamations\n",
      "T-test: t = -39.1612  p-value = 0\n",
      "total_words\n",
      "T-test: t = 22.6859  p-value = 1.44487e-113\n",
      "total_stop_words\n",
      "T-test: t = 18.1066  p-value = 3.99377e-73\n",
      "stop_words_ratio\n",
      "T-test: t = -38.7676  p-value = 0\n",
      "punct_count_norm\n",
      "T-test: t = -18.7714  p-value = 2.59673e-78\n",
      "norm_num_exclamations\n",
      "T-test: t = -27.6515  p-value = 6.43773e-167\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for +: 'dict' and 'dict'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[39], line 14\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[39m# perform t-test on numerical columns\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[39mfor\u001b[39;00m column \u001b[39min\u001b[39;00m [\u001b[39m'\u001b[39m\u001b[39mstop_words_count\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mword_count\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mcap_count\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mpunct_count\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mnum_exclamations\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mtotal_words\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mtotal_stop_words\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mstop_words_ratio\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mpunct_count_norm\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mnorm_num_exclamations\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39msentiment_scores\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39msentiment_scores_num\u001b[39m\u001b[39m'\u001b[39m]:\n\u001b[0;32m---> 14\u001b[0m     t, p \u001b[39m=\u001b[39m stats\u001b[39m.\u001b[39;49mttest_ind(true_df[column], false_df[column], equal_var\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m, nan_policy\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39momit\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[1;32m     15\u001b[0m     \u001b[39mprint\u001b[39m(column)\n\u001b[1;32m     16\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mT-test: t = \u001b[39m\u001b[39m%g\u001b[39;00m\u001b[39m  p-value = \u001b[39m\u001b[39m%g\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m (t, p))\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/scipy/stats/_stats_py.py:6818\u001b[0m, in \u001b[0;36mttest_ind\u001b[0;34m(a, b, axis, equal_var, nan_policy, permutations, random_state, alternative, trim)\u001b[0m\n\u001b[1;32m   6815\u001b[0m n2 \u001b[39m=\u001b[39m b\u001b[39m.\u001b[39mshape[axis]\n\u001b[1;32m   6817\u001b[0m \u001b[39mif\u001b[39;00m trim \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m-> 6818\u001b[0m     v1 \u001b[39m=\u001b[39m _var(a, axis, ddof\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m)\n\u001b[1;32m   6819\u001b[0m     v2 \u001b[39m=\u001b[39m _var(b, axis, ddof\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m   6820\u001b[0m     m1 \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mmean(a, axis)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/scipy/stats/_stats_py.py:1211\u001b[0m, in \u001b[0;36m_var\u001b[0;34m(x, axis, ddof, mean)\u001b[0m\n\u001b[1;32m   1209\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_var\u001b[39m(x, axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m, ddof\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m, mean\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m   1210\u001b[0m     \u001b[39m# Calculate variance of sample, warning if precision is lost\u001b[39;00m\n\u001b[0;32m-> 1211\u001b[0m     var \u001b[39m=\u001b[39m _moment(x, \u001b[39m2\u001b[39;49m, axis, mean\u001b[39m=\u001b[39;49mmean)\n\u001b[1;32m   1212\u001b[0m     \u001b[39mif\u001b[39;00m ddof \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m   1213\u001b[0m         n \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mshape[axis] \u001b[39mif\u001b[39;00m axis \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m x\u001b[39m.\u001b[39msize\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/scipy/stats/_stats_py.py:1181\u001b[0m, in \u001b[0;36m_moment\u001b[0;34m(a, moment, axis, mean)\u001b[0m\n\u001b[1;32m   1178\u001b[0m     n_list\u001b[39m.\u001b[39mappend(current_n)\n\u001b[1;32m   1180\u001b[0m \u001b[39m# Starting point for exponentiation by squares\u001b[39;00m\n\u001b[0;32m-> 1181\u001b[0m mean \u001b[39m=\u001b[39m a\u001b[39m.\u001b[39;49mmean(axis, keepdims\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m) \u001b[39mif\u001b[39;00m mean \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m mean\n\u001b[1;32m   1182\u001b[0m a_zero_mean \u001b[39m=\u001b[39m a \u001b[39m-\u001b[39m mean\n\u001b[1;32m   1184\u001b[0m eps \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mfinfo(a_zero_mean\u001b[39m.\u001b[39mdtype)\u001b[39m.\u001b[39mresolution \u001b[39m*\u001b[39m \u001b[39m10\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/numpy/core/_methods.py:180\u001b[0m, in \u001b[0;36m_mean\u001b[0;34m(a, axis, dtype, out, keepdims, where)\u001b[0m\n\u001b[1;32m    177\u001b[0m         dtype \u001b[39m=\u001b[39m mu\u001b[39m.\u001b[39mdtype(\u001b[39m'\u001b[39m\u001b[39mf4\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m    178\u001b[0m         is_float16_result \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m--> 180\u001b[0m ret \u001b[39m=\u001b[39m umr_sum(arr, axis, dtype, out, keepdims, where\u001b[39m=\u001b[39;49mwhere)\n\u001b[1;32m    181\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(ret, mu\u001b[39m.\u001b[39mndarray):\n\u001b[1;32m    182\u001b[0m     ret \u001b[39m=\u001b[39m um\u001b[39m.\u001b[39mtrue_divide(\n\u001b[1;32m    183\u001b[0m             ret, rcount, out\u001b[39m=\u001b[39mret, casting\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39munsafe\u001b[39m\u001b[39m'\u001b[39m, subok\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for +: 'dict' and 'dict'"
     ]
    }
   ],
   "source": [
    "from scipy import stats\n",
    "import numpy as np\n",
    "# drop rows with missing or invalid values\n",
    "combined_df = combined_df.dropna()\n",
    "combined_df = combined_df.replace([np.inf, -np.inf], np.nan)\n",
    "combined_df = combined_df.dropna()\n",
    "\n",
    "# separate true and false dataframes\n",
    "true_df = combined_df[combined_df['Label'] == True]\n",
    "false_df = combined_df[combined_df['Label'] == False]\n",
    "\n",
    "# perform t-test on numerical columns\n",
    "for column in ['stop_words_count', 'word_count', 'cap_count', 'punct_count', 'num_exclamations', 'total_words', 'total_stop_words', 'stop_words_ratio', 'punct_count_norm', 'norm_num_exclamations', 'sentiment_scores', 'sentiment_scores_num']:\n",
    "    t, p = stats.ttest_ind(true_df[column], false_df[column], equal_var=False, nan_policy='omit')\n",
    "    print(column)\n",
    "    print(\"T-test: t = %g  p-value = %g\" % (t, p)) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# value insights\n",
    "\n",
    "stop_words_count: The mean of stop words count is significantly higher for false news compared to true news.\n",
    "word_count: The mean of word count is significantly higher for false news compared to true news.\n",
    "punct_count: The mean of punctuation count is significantly higher for false news compared to true news.\n",
    "num_exclamations: The mean of the number of exclamation marks is significantly higher for true news compared to false news.\n",
    "total_words: The mean of total words is significantly higher for false news compared to true news.\n",
    "total_stop_words: The mean of total stop words is significantly higher for false news compared to true news.\n",
    "punct_count_norm: The mean of normalized punctuation count is significantly higher for true news compared to false news.\n",
    "norm_num_exclamations: The mean of normalized number of exclamation marks is significantly higher for true news compared to false news.\n",
    "These results suggest that false news articles tend to have more stop words, more words, more punctuation, and fewer total words than true news articles. On the other hand, true news articles tend to have more exclamation marks (both absolute and normalized) than false news articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9367209361485627\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(combined_df['text'], combined_df['Label'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a TF-IDF Vectorizer object\n",
    "tfidf = TfidfVectorizer(stop_words='english')\n",
    "\n",
    "# Fit the Vectorizer on the training data\n",
    "X_train_tfidf = tfidf.fit_transform(X_train)\n",
    "\n",
    "# Train a Logistic Regression classifier on the TF-IDF features\n",
    "clf = LogisticRegression()\n",
    "clf.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Transform the test data into TF-IDF features\n",
    "X_test_tfidf = tfidf.transform(X_test)\n",
    "\n",
    "# Evaluate the classifier on the test data\n",
    "accuracy = clf.score(X_test_tfidf, y_test)\n",
    "print(f\"Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 20 features:\n",
      "said\n",
      "follow\n",
      "thursday\n",
      "tuesday\n",
      "wednesday\n",
      "monday\n",
      "friday\n",
      "twitter\n",
      "breitbart\n",
      "sunday\n",
      "2017\n",
      "reuters\n",
      "saturday\n",
      "ms\n",
      "mr\n",
      "edt\n",
      "minister\n",
      "reporters\n",
      "statement\n",
      "spokesman\n"
     ]
    }
   ],
   "source": [
    "# Get the coefficients of the features\n",
    "coefficients = clf.coef_[0]\n",
    "\n",
    "# Get the indices that sort the coefficients\n",
    "sorted_indices = coefficients.argsort()[::-1]\n",
    "\n",
    "# Get the top 10 feature names\n",
    "top_feature_names = [tfidf.get_feature_names_out()[i] for i in sorted_indices[:20]]\n",
    "\n",
    "# Print the top 10 feature names\n",
    "print(\"Top 20 features:\")\n",
    "for feature in top_feature_names:\n",
    "    print(feature)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>Label</th>\n",
       "      <th>stop_words_count</th>\n",
       "      <th>word_count</th>\n",
       "      <th>cap_count</th>\n",
       "      <th>punct_count</th>\n",
       "      <th>num_exclamations</th>\n",
       "      <th>cap_words_summary</th>\n",
       "      <th>total_words</th>\n",
       "      <th>total_stop_words</th>\n",
       "      <th>stop_words_ratio</th>\n",
       "      <th>punct_count_norm</th>\n",
       "      <th>norm_num_exclamations</th>\n",
       "      <th>sentiment_scores</th>\n",
       "      <th>sentiment_scores_num</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The head of a conservative Republican faction ...</td>\n",
       "      <td>True</td>\n",
       "      <td>303</td>\n",
       "      <td>749</td>\n",
       "      <td>144</td>\n",
       "      <td>115</td>\n",
       "      <td>0</td>\n",
       "      <td>144</td>\n",
       "      <td>749</td>\n",
       "      <td>303</td>\n",
       "      <td>0.404539</td>\n",
       "      <td>0.153538</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>{'neg': 0.048, 'neu': 0.866, 'pos': 0.086, 'co...</td>\n",
       "      <td>0.9847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Transgender people will be allowed for the fir...</td>\n",
       "      <td>True</td>\n",
       "      <td>171</td>\n",
       "      <td>396</td>\n",
       "      <td>72</td>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "      <td>72</td>\n",
       "      <td>396</td>\n",
       "      <td>171</td>\n",
       "      <td>0.431818</td>\n",
       "      <td>0.101010</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>{'neg': 0.063, 'neu': 0.837, 'pos': 0.101, 'co...</td>\n",
       "      <td>0.9496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The special counsel investigation of links bet...</td>\n",
       "      <td>True</td>\n",
       "      <td>201</td>\n",
       "      <td>462</td>\n",
       "      <td>106</td>\n",
       "      <td>44</td>\n",
       "      <td>0</td>\n",
       "      <td>106</td>\n",
       "      <td>462</td>\n",
       "      <td>201</td>\n",
       "      <td>0.435065</td>\n",
       "      <td>0.095238</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>{'neg': 0.049, 'neu': 0.907, 'pos': 0.044, 'co...</td>\n",
       "      <td>-0.6808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Trump campaign adviser George Papadopoulos tol...</td>\n",
       "      <td>True</td>\n",
       "      <td>153</td>\n",
       "      <td>373</td>\n",
       "      <td>99</td>\n",
       "      <td>48</td>\n",
       "      <td>0</td>\n",
       "      <td>99</td>\n",
       "      <td>373</td>\n",
       "      <td>153</td>\n",
       "      <td>0.410188</td>\n",
       "      <td>0.128686</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>{'neg': 0.066, 'neu': 0.875, 'pos': 0.059, 'co...</td>\n",
       "      <td>-0.2201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>President Donald Trump called on the U.S. Post...</td>\n",
       "      <td>True</td>\n",
       "      <td>348</td>\n",
       "      <td>860</td>\n",
       "      <td>182</td>\n",
       "      <td>132</td>\n",
       "      <td>2</td>\n",
       "      <td>182</td>\n",
       "      <td>862</td>\n",
       "      <td>350</td>\n",
       "      <td>0.406032</td>\n",
       "      <td>0.153488</td>\n",
       "      <td>0.002326</td>\n",
       "      <td>{'neg': 0.043, 'neu': 0.898, 'pos': 0.059, 'co...</td>\n",
       "      <td>0.6683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78612</th>\n",
       "      <td>The USA wants to divide Syria.\\r\\n\\r\\nGreat Br...</td>\n",
       "      <td>False</td>\n",
       "      <td>36</td>\n",
       "      <td>70</td>\n",
       "      <td>26</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>26</td>\n",
       "      <td>70</td>\n",
       "      <td>36</td>\n",
       "      <td>0.514286</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>{'neg': 0.0, 'neu': 0.901, 'pos': 0.099, 'comp...</td>\n",
       "      <td>0.7650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78613</th>\n",
       "      <td>The Ukrainian coup d'etat cost the US nothing ...</td>\n",
       "      <td>False</td>\n",
       "      <td>40</td>\n",
       "      <td>106</td>\n",
       "      <td>38</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>38</td>\n",
       "      <td>106</td>\n",
       "      <td>40</td>\n",
       "      <td>0.377358</td>\n",
       "      <td>0.122642</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>{'neg': 0.082, 'neu': 0.891, 'pos': 0.026, 'co...</td>\n",
       "      <td>-0.6492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78614</th>\n",
       "      <td>The European Parliament falsifies history by d...</td>\n",
       "      <td>False</td>\n",
       "      <td>61</td>\n",
       "      <td>129</td>\n",
       "      <td>28</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>28</td>\n",
       "      <td>129</td>\n",
       "      <td>61</td>\n",
       "      <td>0.472868</td>\n",
       "      <td>0.124031</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>{'neg': 0.146, 'neu': 0.796, 'pos': 0.058, 'co...</td>\n",
       "      <td>-0.9217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78615</th>\n",
       "      <td>The European Parliament falsifies history by d...</td>\n",
       "      <td>False</td>\n",
       "      <td>61</td>\n",
       "      <td>129</td>\n",
       "      <td>28</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>28</td>\n",
       "      <td>129</td>\n",
       "      <td>61</td>\n",
       "      <td>0.472868</td>\n",
       "      <td>0.124031</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>{'neg': 0.146, 'neu': 0.796, 'pos': 0.058, 'co...</td>\n",
       "      <td>-0.9217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78616</th>\n",
       "      <td>A leading FSB officer, Segey Beseda, said duri...</td>\n",
       "      <td>False</td>\n",
       "      <td>24</td>\n",
       "      <td>55</td>\n",
       "      <td>26</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>26</td>\n",
       "      <td>55</td>\n",
       "      <td>24</td>\n",
       "      <td>0.436364</td>\n",
       "      <td>0.109091</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>{'neg': 0.066, 'neu': 0.835, 'pos': 0.1, 'comp...</td>\n",
       "      <td>0.2732</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>78617 rows × 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  Label  \\\n",
       "0      The head of a conservative Republican faction ...   True   \n",
       "1      Transgender people will be allowed for the fir...   True   \n",
       "2      The special counsel investigation of links bet...   True   \n",
       "3      Trump campaign adviser George Papadopoulos tol...   True   \n",
       "4      President Donald Trump called on the U.S. Post...   True   \n",
       "...                                                  ...    ...   \n",
       "78612  The USA wants to divide Syria.\\r\\n\\r\\nGreat Br...  False   \n",
       "78613  The Ukrainian coup d'etat cost the US nothing ...  False   \n",
       "78614  The European Parliament falsifies history by d...  False   \n",
       "78615  The European Parliament falsifies history by d...  False   \n",
       "78616  A leading FSB officer, Segey Beseda, said duri...  False   \n",
       "\n",
       "       stop_words_count  word_count  cap_count  punct_count  num_exclamations  \\\n",
       "0                   303         749        144          115                 0   \n",
       "1                   171         396         72           40                 0   \n",
       "2                   201         462        106           44                 0   \n",
       "3                   153         373         99           48                 0   \n",
       "4                   348         860        182          132                 2   \n",
       "...                 ...         ...        ...          ...               ...   \n",
       "78612                36          70         26           14                 0   \n",
       "78613                40         106         38           13                 0   \n",
       "78614                61         129         28           16                 0   \n",
       "78615                61         129         28           16                 0   \n",
       "78616                24          55         26            6                 0   \n",
       "\n",
       "       cap_words_summary  total_words  total_stop_words  stop_words_ratio  \\\n",
       "0                    144          749               303          0.404539   \n",
       "1                     72          396               171          0.431818   \n",
       "2                    106          462               201          0.435065   \n",
       "3                     99          373               153          0.410188   \n",
       "4                    182          862               350          0.406032   \n",
       "...                  ...          ...               ...               ...   \n",
       "78612                 26           70                36          0.514286   \n",
       "78613                 38          106                40          0.377358   \n",
       "78614                 28          129                61          0.472868   \n",
       "78615                 28          129                61          0.472868   \n",
       "78616                 26           55                24          0.436364   \n",
       "\n",
       "       punct_count_norm  norm_num_exclamations  \\\n",
       "0              0.153538               0.000000   \n",
       "1              0.101010               0.000000   \n",
       "2              0.095238               0.000000   \n",
       "3              0.128686               0.000000   \n",
       "4              0.153488               0.002326   \n",
       "...                 ...                    ...   \n",
       "78612          0.200000               0.000000   \n",
       "78613          0.122642               0.000000   \n",
       "78614          0.124031               0.000000   \n",
       "78615          0.124031               0.000000   \n",
       "78616          0.109091               0.000000   \n",
       "\n",
       "                                        sentiment_scores  sentiment_scores_num  \n",
       "0      {'neg': 0.048, 'neu': 0.866, 'pos': 0.086, 'co...                0.9847  \n",
       "1      {'neg': 0.063, 'neu': 0.837, 'pos': 0.101, 'co...                0.9496  \n",
       "2      {'neg': 0.049, 'neu': 0.907, 'pos': 0.044, 'co...               -0.6808  \n",
       "3      {'neg': 0.066, 'neu': 0.875, 'pos': 0.059, 'co...               -0.2201  \n",
       "4      {'neg': 0.043, 'neu': 0.898, 'pos': 0.059, 'co...                0.6683  \n",
       "...                                                  ...                   ...  \n",
       "78612  {'neg': 0.0, 'neu': 0.901, 'pos': 0.099, 'comp...                0.7650  \n",
       "78613  {'neg': 0.082, 'neu': 0.891, 'pos': 0.026, 'co...               -0.6492  \n",
       "78614  {'neg': 0.146, 'neu': 0.796, 'pos': 0.058, 'co...               -0.9217  \n",
       "78615  {'neg': 0.146, 'neu': 0.796, 'pos': 0.058, 'co...               -0.9217  \n",
       "78616  {'neg': 0.066, 'neu': 0.835, 'pos': 0.1, 'comp...                0.2732  \n",
       "\n",
       "[78617 rows x 15 columns]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Get the coefficients of the features\n",
    "coefficients = clf.coef_[0]\n",
    "\n",
    "# Get the indices that sort the coefficients\n",
    "sorted_indices = coefficients.argsort()\n",
    "\n",
    "# Get the top 10 feature names and coefficients\n",
    "top_feature_names = [tfidf.get_feature_names_out()[i] for i in sorted_indices[-10:]]\n",
    "top_coefficients = coefficients[sorted_indices][-10:]\n",
    "\n",
    "# Create a horizontal bar chart\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "y_pos = np.arange(len(top_feature_names))\n",
    "ax.barh(y_pos, top_coefficients, align='center')\n",
    "ax.set_yticks(y_pos)\n",
    "ax.set_yticklabels(top_feature_names)\n",
    "ax.invert_yaxis()\n",
    "ax.set_xlabel('Coefficient')\n",
    "ax.set_title('Top 10 Indicative Features of True or False News')\n",
    "\n",
    "# Save the plot as a PNG file\n",
    "plt.savefig('top_features.png', dpi=300, bbox_inches='tight')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "N-grams: You could extract n-grams (groups of n consecutive words) from the news body column and compare the frequency of different n-grams between true and false news.\n",
    "\n",
    "Readability Metrics: Readability metrics are measures of how easy or difficult it is to read a piece of text. By calculating readability metrics such as Flesch-Kincaid Grade Level, Gunning Fog Index, or Coleman-Liau Index, you could compare the readability of true and false news. For example, if false news articles tend to have a lower readability score than true news articles, that could be an indicator of a trend.\n",
    "\n",
    "Grammatical Errors: By analyzing the news body column for grammatical errors such as spelling mistakes, incorrect verb tense, or subject-verb agreement errors, you could compare the frequency of errors between true and false news. For example, if false news articles tend to have more grammatical errors than true news articles, that could be an indicator of a trend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most common n-grams for true news:\n",
      "[('the United States', 11412), ('said in a', 6921), ('one of the', 6495), ('the White House', 4727), ('President Donald Trump', 4230), ('as well as', 3751), ('in the United', 3636), ('a lot of', 3076), ('the end of', 3011), ('according to the', 2890)]\n",
      "Most common n-grams for false news:\n",
      "[('the United States', 6619), ('one of the', 6151), ('as well as', 3825), ('the fact that', 3793), ('a lot of', 3538), ('the White House', 3435), ('in order to', 3333), ('of the United', 3064), ('to be a', 2879), ('out of the', 2770)]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from nltk import ngrams\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "# Define the n-gram size\n",
    "n = 3\n",
    "\n",
    "# Define a function to extract n-grams from a string\n",
    "def get_ngrams(text):\n",
    "    # Tokenize the text into words\n",
    "    words = text.split()\n",
    "    # Extract n-grams\n",
    "    ngrams_list = ngrams(words, n)\n",
    "    # Convert the n-grams to strings\n",
    "    ngrams_strings = [' '.join(ngram) for ngram in ngrams_list]\n",
    "    return ngrams_strings\n",
    "\n",
    "# Define a dictionary to hold the n-gram frequencies for true and false news\n",
    "ngrams_freq = {True: Counter(), False: Counter()}\n",
    "\n",
    "# Loop through each row in the DataFrame\n",
    "for index, row in combined_df.iterrows():\n",
    "    # Extract the news body and label from the row\n",
    "    text = row['text']\n",
    "    label = row['Label']\n",
    "    # Extract the n-grams from the news body\n",
    "    ngrams_list = get_ngrams(text)\n",
    "    # Update the n-gram frequencies for the corresponding label\n",
    "    ngrams_freq[label].update(ngrams_list)\n",
    "\n",
    "# Print the most common n-grams for true and false news\n",
    "print('Most common n-grams for true news:')\n",
    "print(ngrams_freq[True].most_common(10))\n",
    "print('Most common n-grams for false news:')\n",
    "print(ngrams_freq[False].most_common(10))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both true and false news tend to have n-grams related to the United States, with \"the United States\" being the most common n-gram for both.\n",
    "True news articles tend to have n-grams related to quotes and statements, with \"said in a\" and \"according to the\" being among the most common n-grams.\n",
    "False news articles tend to have n-grams related to opinions and speculation, with \"the fact that\" being among the most common n-grams.\n",
    "Both true and false news tend to have n-grams related to the White House, with \"the White House\" being among the most common n-grams for both.\n",
    "Both true and false news articles tend to have n-grams related to government and politics, with \"in the United\" and \"of the United\" being among the most common n-grams for both.\n",
    "False news articles tend to have more sensational or provocative n-grams, such as \"to be a\" and \"out of the\". This suggests that false news may be more likely to use exaggerated or attention-grabbing language.\n",
    "True news articles tend to have more specific n-grams related to events or actions, such as \"the end of\" and \"took place on\". This suggests that true news may provide more detailed and factual information.\n",
    "Both true and false news articles tend to use phrases like \"as well as\" and \"a lot of\", indicating that these are common filler phrases in news writing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Flesch-Kincaid Grade Level for true news: 10.91\n",
      "Average Flesch-Kincaid Grade Level for false news: 10.35\n",
      "Average Gunning Fog Index for true news: 12.42\n",
      "Average Gunning Fog Index for false news: 11.83\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import textstat\n",
    "\n",
    "# Define a function to calculate the readability metrics for a given text\n",
    "def get_readability_metrics(text):\n",
    "    fk_grade_level = textstat.flesch_kincaid_grade(text)\n",
    "    gf_index = textstat.gunning_fog(text)\n",
    "    return {'fk_grade_level': fk_grade_level, 'gf_index': gf_index}\n",
    "\n",
    "# Create new columns for the readability metrics\n",
    "combined_df['fk_grade_level'] = combined_df['text'].apply(textstat.flesch_kincaid_grade)\n",
    "combined_df['gf_index'] = combined_df['text'].apply(textstat.gunning_fog)\n",
    "\n",
    "# Calculate the average readability metrics for true and false news\n",
    "true_fk_grade_level_avg = combined_df[combined_df['Label'] == True]['fk_grade_level'].mean()\n",
    "false_fk_grade_level_avg = combined_df[combined_df['Label'] == False]['fk_grade_level'].mean()\n",
    "true_gf_index_avg = combined_df[combined_df['Label'] == True]['gf_index'].mean()\n",
    "false_gf_index_avg = combined_df[combined_df['Label'] == False]['gf_index'].mean()\n",
    "\n",
    "# Print the results\n",
    "print('Average Flesch-Kincaid Grade Level for true news: {:.2f}'.format(true_fk_grade_level_avg))\n",
    "print('Average Flesch-Kincaid Grade Level for false news: {:.2f}'.format(false_fk_grade_level_avg))\n",
    "print('Average Gunning Fog Index for true news: {:.2f}'.format(true_gf_index_avg))\n",
    "print('Average Gunning Fog Index for false news: {:.2f}'.format(false_gf_index_avg))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Average Flesch-Kincaid Grade Level for true news: 10.91\n",
    "Average Flesch-Kincaid Grade Level for false news: 10.35\n",
    "Average Gunning Fog Index for true news: 12.42\n",
    "Average Gunning Fog Index for false news: 11.83\n",
    "\n",
    "Based on the average Flesch-Kincaid Grade Level and Gunning Fog Index values, it seems that there is not a significant difference in the readability of true and false news articles in the dataset. Both true and false news articles have similar scores, indicating that they may be similarly difficult to read."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyspellchecker\n",
      "  Downloading pyspellchecker-0.7.1-py3-none-any.whl (2.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m0m\n",
      "\u001b[?25hInstalling collected packages: pyspellchecker\n",
      "Successfully installed pyspellchecker-0.7.1\n"
     ]
    }
   ],
   "source": [
    "from spellchecker import SpellChecker\n",
    "\n",
    "# create a spellchecker object\n",
    "spell = SpellChecker()\n",
    "\n",
    "# define a function to count the number of misspelled words in a string\n",
    "def count_misspelled_words(text):\n",
    "    # split the text into words\n",
    "    words = text.split()\n",
    "    # count the number of misspelled words\n",
    "    misspelled = spell.unknown(words)\n",
    "    count = len(misspelled)\n",
    "    return count\n",
    "\n",
    "# apply the count_misspelled_words function to the news body column\n",
    "combined_df['misspelled_words'] = combined_df['text'].apply(count_misspelled_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average number of misspelled words for true news: 69.97415296640457\n",
      "Average number of misspelled words for false news: 57.62813344942945\n"
     ]
    }
   ],
   "source": [
    "# calculate the average number of misspelled words for true and false news articles\n",
    "avg_misspelled_words_true = combined_df.loc[combined_df['Label'] == True, 'misspelled_words'].mean()\n",
    "avg_misspelled_words_false = combined_df.loc[combined_df['Label'] == False, 'misspelled_words'].mean()\n",
    "\n",
    "print('Average number of misspelled words for true news:', avg_misspelled_words_true)\n",
    "print('Average number of misspelled words for false news:', avg_misspelled_words_false)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average number of grammar errors for true news: 0.00\n",
      "Average number of grammar errors for false news: 0.00\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Load the English language model in spaCy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Define a function to count the number of grammatical errors in a text\n",
    "def count_grammar_errors(text):\n",
    "    # Parse the text with spaCy\n",
    "    doc = nlp(text)\n",
    "    # Count the number of verb tense errors and subject-verb agreement errors\n",
    "    num_errors = sum([token.tag_.startswith('V') and token.is_alpha and not token.is_stop and not token.is_oov and not token.is_punct and not token.is_digit and not token.like_num and (token.dep_ == 'aux' or token.dep_ == 'auxpass' or token.dep_ == 'VERB' and not token.head.tag_.startswith('V')) for token in doc])\n",
    "    num_errors += sum([token.tag_ == 'VBZ' and token.is_alpha and not token.is_stop and not token.is_oov and not token.is_punct and not token.is_digit and not token.like_num and (token.dep_ == 'nsubj' or token.dep_ == 'nsubjpass') for token in doc])\n",
    "    return num_errors\n",
    "\n",
    "# Calculate the average number of grammar errors for true and false news\n",
    "num_grammar_errors_true = [count_grammar_errors(text) for text in true_df['text']]\n",
    "num_grammar_errors_false = [count_grammar_errors(text) for text in false_df['text']]\n",
    "avg_num_grammar_errors_true = sum(num_grammar_errors_true) / len(num_grammar_errors_true)\n",
    "avg_num_grammar_errors_false = sum(num_grammar_errors_false) / len(num_grammar_errors_false)\n",
    "\n",
    "print(f'Average number of grammar errors for true news: {avg_num_grammar_errors_true:.2f}')\n",
    "print(f'Average number of grammar errors for false news: {avg_num_grammar_errors_false:.2f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(('Trump', 'ORG'), 50667), (('U.S.', 'GPE'), 30168), (('Trump', 'PERSON'), 23460), (('Republican', 'NORP'), 21507), (('Clinton', 'PERSON'), 18792), (('first', 'ORDINAL'), 17332), (('the United States', 'GPE'), 16807), (('one', 'CARDINAL'), 16777), (('two', 'CARDINAL'), 16559), (('Donald Trump', 'PERSON'), 13515)]\n",
      "[(('Trump', 'ORG'), 61723), (('Clinton', 'PERSON'), 21143), (('Donald Trump', 'PERSON'), 19017), (('US', 'GPE'), 17524), (('Russia', 'GPE'), 17316), (('one', 'CARDINAL'), 17047), (('American', 'NORP'), 15247), (('Hillary Clinton', 'PERSON'), 15103), (('America', 'GPE'), 14101), (('first', 'ORDINAL'), 13302)]\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Load spaCy's English language model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Define a function to extract named entities from text\n",
    "def extract_entities(text):\n",
    "    doc = nlp(text)\n",
    "    entities = []\n",
    "    for ent in doc.ents:\n",
    "        entities.append((ent.text, ent.label_))\n",
    "    return entities\n",
    "\n",
    "# Apply the function to the 'text' column of the combined_df dataframe\n",
    "combined_df['entities'] = combined_df['text'].apply(extract_entities)\n",
    "\n",
    "# Print the top 10 most common named entities for true news\n",
    "true_entities = combined_df[combined_df['Label'] == True]['entities'].sum()\n",
    "true_entity_counts = Counter(true_entities)\n",
    "print(true_entity_counts.most_common(10))\n",
    "\n",
    "# Print the top 10 most common named entities for false news\n",
    "false_entities = combined_df[combined_df['Label'] == False]['entities'].sum()\n",
    "false_entity_counts = Counter(false_entities)\n",
    "print(false_entity_counts.most_common(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
